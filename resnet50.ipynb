{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "resnet50.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOACAEGIr1lKmBgX0Q4+E4K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leehon94/SwAV/blob/main/resnet50.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4Dx4eRuzBYH"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(\n",
        "        in_planes,\n",
        "        out_planes,\n",
        "        kernel_size=3,\n",
        "        stride=stride,\n",
        "        padding=dilation,\n",
        "        groups=groups,\n",
        "        bias=False,\n",
        "        dilation=dilation,\n",
        "    )\n",
        "\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "    __constants__ = [\"downsample\"]\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        inplanes,\n",
        "        planes,\n",
        "        stride=1,\n",
        "        downsample=None,\n",
        "        groups=1,\n",
        "        base_width=64,\n",
        "        dilation=1,\n",
        "        norm_layer=None,\n",
        "    ):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        if groups != 1 or base_width != 64:\n",
        "            raise ValueError(\"BasicBlock only supports groups=1 and base_width=64\")\n",
        "        if dilation > 1:\n",
        "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
        "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "    __constants__ = [\"downsample\"]\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        inplanes,\n",
        "        planes,\n",
        "        stride=1,\n",
        "        downsample=None,\n",
        "        groups=1,\n",
        "        base_width=64,\n",
        "        dilation=1,\n",
        "        norm_layer=None,\n",
        "    ):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        width = int(planes * (base_width / 64.0)) * groups\n",
        "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv1x1(inplanes, width)\n",
        "        self.bn1 = norm_layer(width)\n",
        "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
        "        self.bn2 = norm_layer(width)\n",
        "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
        "        self.bn3 = norm_layer(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            block,\n",
        "            layers,\n",
        "            zero_init_residual=False,\n",
        "            groups=1,\n",
        "            widen=1,\n",
        "            width_per_group=64,\n",
        "            replace_stride_with_dilation=None,\n",
        "            norm_layer=None,\n",
        "            normalize=False,\n",
        "            output_dim=0,\n",
        "            hidden_mlp=0,\n",
        "            nmb_prototypes=0,\n",
        "            eval_mode=False,\n",
        "    ):\n",
        "        super(ResNet, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        self._norm_layer = norm_layer\n",
        "\n",
        "        self.eval_mode = eval_mode\n",
        "        self.padding = nn.ConstantPad2d(1, 0.0)\n",
        "\n",
        "        self.inplanes = width_per_group * widen\n",
        "        self.dilation = 1\n",
        "        if replace_stride_with_dilation is None:\n",
        "            # each element in the tuple indicates if we should replace\n",
        "            # the 2x2 stride with a dilated convolution instead\n",
        "            replace_stride_with_dilation = [False, False, False]\n",
        "        if len(replace_stride_with_dilation) != 3:\n",
        "            raise ValueError(\n",
        "                \"replace_stride_with_dilation should be None \"\n",
        "                \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation)\n",
        "            )\n",
        "        self.groups = groups\n",
        "        self.base_width = width_per_group\n",
        "\n",
        "        # change padding 3 -> 2 compared to original torchvision code because added a padding layer\n",
        "        num_out_filters = width_per_group * widen\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            3, num_out_filters, kernel_size=7, stride=2, padding=2, bias=False\n",
        "        )\n",
        "        self.bn1 = norm_layer(num_out_filters)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, num_out_filters, layers[0])\n",
        "        num_out_filters *= 2\n",
        "        self.layer2 = self._make_layer(\n",
        "            block, num_out_filters, layers[1], stride=2, dilate=replace_stride_with_dilation[0]\n",
        "        )\n",
        "        num_out_filters *= 2\n",
        "        self.layer3 = self._make_layer(\n",
        "            block, num_out_filters, layers[2], stride=2, dilate=replace_stride_with_dilation[1]\n",
        "        )\n",
        "        num_out_filters *= 2\n",
        "        self.layer4 = self._make_layer(\n",
        "            block, num_out_filters, layers[3], stride=2, dilate=replace_stride_with_dilation[2]\n",
        "        )\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        # normalize output features\n",
        "        self.l2norm = normalize\n",
        "\n",
        "        # projection head\n",
        "        if output_dim == 0:\n",
        "            self.projection_head = None\n",
        "        elif hidden_mlp == 0:\n",
        "            self.projection_head = nn.Linear(num_out_filters * block.expansion, output_dim)\n",
        "        else:\n",
        "            self.projection_head = nn.Sequential(\n",
        "                nn.Linear(num_out_filters * block.expansion, hidden_mlp),\n",
        "                nn.BatchNorm1d(hidden_mlp),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Linear(hidden_mlp, output_dim),\n",
        "            )\n",
        "\n",
        "        # prototype layer\n",
        "        self.prototypes = None\n",
        "        if isinstance(nmb_prototypes, list):\n",
        "            self.prototypes = MultiPrototypes(output_dim, nmb_prototypes)\n",
        "        elif nmb_prototypes > 0:\n",
        "            self.prototypes = nn.Linear(output_dim, nmb_prototypes, bias=False)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
        "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, Bottleneck):\n",
        "                    nn.init.constant_(m.bn3.weight, 0)\n",
        "                elif isinstance(m, BasicBlock):\n",
        "                    nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
        "        norm_layer = self._norm_layer\n",
        "        downsample = None\n",
        "        previous_dilation = self.dilation\n",
        "        if dilate:\n",
        "            self.dilation *= stride\n",
        "            stride = 1\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
        "                norm_layer(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(\n",
        "            block(\n",
        "                self.inplanes,\n",
        "                planes,\n",
        "                stride,\n",
        "                downsample,\n",
        "                self.groups,\n",
        "                self.base_width,\n",
        "                previous_dilation,\n",
        "                norm_layer,\n",
        "            )\n",
        "        )\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(\n",
        "                block(\n",
        "                    self.inplanes,\n",
        "                    planes,\n",
        "                    groups=self.groups,\n",
        "                    base_width=self.base_width,\n",
        "                    dilation=self.dilation,\n",
        "                    norm_layer=norm_layer,\n",
        "                )\n",
        "            )\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward_backbone(self, x):\n",
        "        x = self.padding(x)\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        if self.eval_mode:\n",
        "            return x\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward_head(self, x):\n",
        "        if self.projection_head is not None:\n",
        "            x = self.projection_head(x)\n",
        "\n",
        "        if self.l2norm:\n",
        "            x = nn.functional.normalize(x, dim=1, p=2)\n",
        "\n",
        "        if self.prototypes is not None:\n",
        "            return x, self.prototypes(x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        if not isinstance(inputs, list):\n",
        "            inputs = [inputs]\n",
        "        idx_crops = torch.cumsum(torch.unique_consecutive(\n",
        "            torch.tensor([inp.shape[-1] for inp in inputs]),\n",
        "            return_counts=True,\n",
        "        )[1], 0)\n",
        "        start_idx = 0\n",
        "        for end_idx in idx_crops:\n",
        "            _out = self.forward_backbone(torch.cat(inputs[start_idx: end_idx]).cuda(non_blocking=True))\n",
        "            if start_idx == 0:\n",
        "                output = _out\n",
        "            else:\n",
        "                output = torch.cat((output, _out))\n",
        "            start_idx = end_idx\n",
        "        return self.forward_head(output)\n",
        "\n",
        "\n",
        "class MultiPrototypes(nn.Module):\n",
        "    def __init__(self, output_dim, nmb_prototypes):\n",
        "        super(MultiPrototypes, self).__init__()\n",
        "        self.nmb_heads = len(nmb_prototypes)\n",
        "        for i, k in enumerate(nmb_prototypes):\n",
        "            self.add_module(\"prototypes\" + str(i), nn.Linear(output_dim, k, bias=False))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = []\n",
        "        for i in range(self.nmb_heads):\n",
        "            out.append(getattr(self, \"prototypes\" + str(i))(x))\n",
        "        return out\n",
        "\n",
        "\n",
        "def resnet50(**kwargs):\n",
        "    return ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n",
        "\n",
        "\n",
        "def resnet50w2(**kwargs):\n",
        "    return ResNet(Bottleneck, [3, 4, 6, 3], widen=2, **kwargs)\n",
        "\n",
        "\n",
        "def resnet50w4(**kwargs):\n",
        "    return ResNet(Bottleneck, [3, 4, 6, 3], widen=4, **kwargs)\n",
        "\n",
        "\n",
        "def resnet50w5(**kwargs):\n",
        "    return ResNet(Bottleneck, [3, 4, 6, 3], widen=5, **kwargs)"
      ],
      "execution_count": 1,
      "outputs": []
    }
  ]
}