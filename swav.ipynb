{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "swav.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOgE0R5TT/oS6OkcZNWUt2b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ed4660e777ee4dd69f673c2af0387ba1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_bedb863ea3b54af29f836ae6c0b2cd60",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_40915b1adc864cd2945c86084be328b7",
              "IPY_MODEL_ec8aa2a6b07b41e6b2302e4cf644a579"
            ]
          }
        },
        "bedb863ea3b54af29f836ae6c0b2cd60": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "40915b1adc864cd2945c86084be328b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_af02afed884d4fcc870576713283b16a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8a30a539e8f54138bf5a45b9607d8903"
          }
        },
        "ec8aa2a6b07b41e6b2302e4cf644a579": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_aa894fc4329c465bbb25b2377436cf42",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2640404480/? [00:50&lt;00:00, 22635190.12it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_dc85f0bc159c41f2bcd6b4b6156995e4"
          }
        },
        "af02afed884d4fcc870576713283b16a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8a30a539e8f54138bf5a45b9607d8903": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "aa894fc4329c465bbb25b2377436cf42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "dc85f0bc159c41f2bcd6b4b6156995e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c14b88f24acf4bcfbfa8e0f34ba417d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ccabec6cf7804577a61e1f4e7f15e75c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6683a03ca4ea44feb1b55ccc32d715d2",
              "IPY_MODEL_2715458a09474dbaae562a9c27341877"
            ]
          }
        },
        "ccabec6cf7804577a61e1f4e7f15e75c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "6683a03ca4ea44feb1b55ccc32d715d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6bd912b70c30484f9044c652d378c16a",
            "_dom_classes": [],
            "description": "Validation sanity check: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 2,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_96c453b0b28f416597939de2eb8bc342"
          }
        },
        "2715458a09474dbaae562a9c27341877": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b4008ae9f1f8475f8c465a15626e9a9b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2/2 [00:21&lt;00:00,  9.86s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3eec7552813d408ea84d039dceac3049"
          }
        },
        "6bd912b70c30484f9044c652d378c16a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "96c453b0b28f416597939de2eb8bc342": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b4008ae9f1f8475f8c465a15626e9a9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3eec7552813d408ea84d039dceac3049": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a64a23f32a0342f2991638209f8c34ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_dc2bf2cfce364867b83a250bb1032da8",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_fb776bb6898d491ba6fd3ab8dccc7eec",
              "IPY_MODEL_3bd955549837458a8e97833f8ba11d65"
            ]
          }
        },
        "dc2bf2cfce364867b83a250bb1032da8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "fb776bb6898d491ba6fd3ab8dccc7eec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_810884e309b54c17be2b7b5ff542492a",
            "_dom_classes": [],
            "description": "Epoch 0:   0%",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 1562,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_62d29179e0ac49ad994ed49504daa1ee"
          }
        },
        "3bd955549837458a8e97833f8ba11d65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_98624cb2c7d244388cc1ebc815eb21c7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/1562 [00:15&lt;?, ?it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6ce4255c08fd4f15ab1929474318c908"
          }
        },
        "810884e309b54c17be2b7b5ff542492a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "62d29179e0ac49ad994ed49504daa1ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "98624cb2c7d244388cc1ebc815eb21c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6ce4255c08fd4f15ab1929474318c908": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leehon94/SwAV/blob/main/swav.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6zErbex-j5k"
      },
      "source": [
        "**Installation von PyTorch Lightning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxu1RiIv-AWW",
        "outputId": "8279b399-47cd-4bd5-f2e6-cb63353dcdf1"
      },
      "source": [
        "!pip install pytorch-lightning\n",
        "!pip install pytorch-lightning-bolts\n",
        "!pip install git+https://github.com/PyTorchLightning/pytorch-lightning --upgrade"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.6/dist-packages (1.1.4)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning) (5.3.1)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning) (0.18.2)\n",
            "Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning) (1.7.0+cu101)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning) (1.19.5)\n",
            "Requirement already satisfied: fsspec[http]>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning) (0.8.5)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning) (2.4.0)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning) (4.41.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.3->pytorch-lightning) (3.7.4.3)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.3->pytorch-lightning) (0.8)\n",
            "Requirement already satisfied: requests; extra == \"http\" in /usr/local/lib/python3.6/dist-packages (from fsspec[http]>=0.8.1->pytorch-lightning) (2.23.0)\n",
            "Requirement already satisfied: aiohttp; extra == \"http\" in /usr/local/lib/python3.6/dist-packages (from fsspec[http]>=0.8.1->pytorch-lightning) (3.7.3)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.32.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.36.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.4.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.10.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (51.1.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.15.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.7.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.3.3)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.12.4)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.17.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.0.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning) (2020.12.5)\n",
            "Requirement already satisfied: async-timeout<4.0,>=3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning) (3.0.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.6/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning) (5.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning) (1.6.3)\n",
            "Requirement already satisfied: idna-ssl>=1.0; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning) (1.1.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning) (20.3.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (3.3.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.2.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.2.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (3.4.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.4.8)\n",
            "Requirement already satisfied: pytorch-lightning-bolts in /usr/local/lib/python3.6/dist-packages (0.2.5)\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning-bolts) (1.7.0+cu101)\n",
            "Requirement already satisfied: pytorch-lightning>=0.10.0 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning-bolts) (1.1.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.6->pytorch-lightning-bolts) (0.18.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch>=1.6->pytorch-lightning-bolts) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.6->pytorch-lightning-bolts) (3.7.4.3)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.6->pytorch-lightning-bolts) (0.8)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning>=0.10.0->pytorch-lightning-bolts) (5.3.1)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning>=0.10.0->pytorch-lightning-bolts) (4.41.1)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning>=0.10.0->pytorch-lightning-bolts) (2.4.0)\n",
            "Requirement already satisfied: fsspec[http]>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning>=0.10.0->pytorch-lightning-bolts) (0.8.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=0.10.0->pytorch-lightning-bolts) (3.3.3)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=0.10.0->pytorch-lightning-bolts) (1.32.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=0.10.0->pytorch-lightning-bolts) (0.4.2)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=0.10.0->pytorch-lightning-bolts) (1.17.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=0.10.0->pytorch-lightning-bolts) (1.15.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=0.10.0->pytorch-lightning-bolts) (2.23.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=0.10.0->pytorch-lightning-bolts) (51.1.1)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=0.10.0->pytorch-lightning-bolts) (0.36.2)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=0.10.0->pytorch-lightning-bolts) (3.12.4)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=0.10.0->pytorch-lightning-bolts) (0.10.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=0.10.0->pytorch-lightning-bolts) (1.7.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=0.10.0->pytorch-lightning-bolts) (1.0.1)\n",
            "Requirement already satisfied: aiohttp; extra == \"http\" in /usr/local/lib/python3.6/dist-packages (from fsspec[http]>=0.8.1->pytorch-lightning>=0.10.0->pytorch-lightning-bolts) (3.7.3)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning>=0.10.0->pytorch-lightning-bolts) (3.3.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning>=0.10.0->pytorch-lightning-bolts) (1.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning>=0.10.0->pytorch-lightning-bolts) (4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning>=0.10.0->pytorch-lightning-bolts) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning>=0.10.0->pytorch-lightning-bolts) (4.2.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch-lightning>=0.10.0->pytorch-lightning-bolts) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch-lightning>=0.10.0->pytorch-lightning-bolts) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch-lightning>=0.10.0->pytorch-lightning-bolts) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch-lightning>=0.10.0->pytorch-lightning-bolts) (2.10)\n",
            "Requirement already satisfied: async-timeout<4.0,>=3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning>=0.10.0->pytorch-lightning-bolts) (3.0.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning>=0.10.0->pytorch-lightning-bolts) (1.6.3)\n",
            "Requirement already satisfied: idna-ssl>=1.0; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning>=0.10.0->pytorch-lightning-bolts) (1.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.6/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning>=0.10.0->pytorch-lightning-bolts) (5.1.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning>=0.10.0->pytorch-lightning-bolts) (20.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning>=0.10.0->pytorch-lightning-bolts) (3.4.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning>=0.10.0->pytorch-lightning-bolts) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning>=0.10.0->pytorch-lightning-bolts) (0.4.8)\n",
            "Collecting git+https://github.com/PyTorchLightning/pytorch-lightning\n",
            "  Cloning https://github.com/PyTorchLightning/pytorch-lightning to /tmp/pip-req-build-yqdc7u_o\n",
            "  Running command git clone -q https://github.com/PyTorchLightning/pytorch-lightning /tmp/pip-req-build-yqdc7u_o\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.16.6 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning==1.1.4) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: fsspec[http]>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning==1.1.4) (0.8.5)\n",
            "Requirement already satisfied, skipping upgrade: PyYAML>=5.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning==1.1.4) (5.3.1)\n",
            "Requirement already satisfied, skipping upgrade: torch>=1.3 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning==1.1.4) (1.7.0+cu101)\n",
            "Requirement already satisfied, skipping upgrade: tqdm>=4.41.0 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning==1.1.4) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: future>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning==1.1.4) (0.18.2)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning==1.1.4) (2.4.0)\n",
            "Requirement already satisfied, skipping upgrade: aiohttp; extra == \"http\" in /usr/local/lib/python3.6/dist-packages (from fsspec[http]>=0.8.1->pytorch-lightning==1.1.4) (3.7.3)\n",
            "Requirement already satisfied, skipping upgrade: requests; extra == \"http\" in /usr/local/lib/python3.6/dist-packages (from fsspec[http]>=0.8.1->pytorch-lightning==1.1.4) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.3->pytorch-lightning==1.1.4) (0.8)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.3->pytorch-lightning==1.1.4) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.1.4) (1.32.0)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.1.4) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.1.4) (1.17.2)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.1.4) (3.3.3)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.1.4) (1.7.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.1.4) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.1.4) (0.36.2)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.1.4) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.1.4) (0.4.2)\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.1.4) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.1.4) (51.1.1)\n",
            "Requirement already satisfied, skipping upgrade: idna-ssl>=1.0; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning==1.1.4) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4.0,>=2.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning==1.1.4) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: attrs>=17.3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning==1.1.4) (20.3.0)\n",
            "Requirement already satisfied, skipping upgrade: multidict<7.0,>=4.5 in /usr/local/lib/python3.6/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning==1.1.4) (5.1.0)\n",
            "Requirement already satisfied, skipping upgrade: async-timeout<4.0,>=3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning==1.1.4) (3.0.1)\n",
            "Requirement already satisfied, skipping upgrade: yarl<2.0,>=1.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning==1.1.4) (1.6.3)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning==1.1.4) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning==1.1.4) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning==1.1.4) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.1.4) (4.2.0)\n",
            "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.1.4) (4.6)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.1.4) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning==1.1.4) (3.3.0)\n",
            "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.1.4) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.1.4) (0.4.8)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning==1.1.4) (3.4.0)\n",
            "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.1.4) (3.1.0)\n",
            "Building wheels for collected packages: pytorch-lightning\n",
            "  Building wheel for pytorch-lightning (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytorch-lightning: filename=pytorch_lightning-1.1.4-cp36-none-any.whl size=683386 sha256=57a569ba396c3096944c69040f26df7c7b22f9da0a7f7c3e0bb89ff446c29a95\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-y1dv6nal/wheels/e2/c6/88/caa5d4cfbfab631fc84b0107896a6f661a1caf589160c27e71\n",
            "Successfully built pytorch-lightning\n",
            "Installing collected packages: pytorch-lightning\n",
            "  Found existing installation: pytorch-lightning 1.1.4\n",
            "    Uninstalling pytorch-lightning-1.1.4:\n",
            "      Successfully uninstalled pytorch-lightning-1.1.4\n",
            "Successfully installed pytorch-lightning-1.1.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4wYZMkU-tWw"
      },
      "source": [
        "#ResNet50 - Architektur SwAV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOE2LyV1-sxq"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(\n",
        "        in_planes,\n",
        "        out_planes,\n",
        "        kernel_size=3,\n",
        "        stride=stride,\n",
        "        padding=dilation,\n",
        "        groups=groups,\n",
        "        bias=False,\n",
        "        dilation=dilation,\n",
        "    )\n",
        "\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "    __constants__ = [\"downsample\"]\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        inplanes,\n",
        "        planes,\n",
        "        stride=1,\n",
        "        downsample=None,\n",
        "        groups=1,\n",
        "        base_width=64,\n",
        "        dilation=1,\n",
        "        norm_layer=None,\n",
        "    ):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        if groups != 1 or base_width != 64:\n",
        "            raise ValueError(\"BasicBlock only supports groups=1 and base_width=64\")\n",
        "        if dilation > 1:\n",
        "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
        "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "    __constants__ = [\"downsample\"]\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        inplanes,\n",
        "        planes,\n",
        "        stride=1,\n",
        "        downsample=None,\n",
        "        groups=1,\n",
        "        base_width=64,\n",
        "        dilation=1,\n",
        "        norm_layer=None,\n",
        "    ):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        width = int(planes * (base_width / 64.0)) * groups\n",
        "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv1x1(inplanes, width)\n",
        "        self.bn1 = norm_layer(width)\n",
        "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
        "        self.bn2 = norm_layer(width)\n",
        "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
        "        self.bn3 = norm_layer(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        block,\n",
        "        layers,\n",
        "        zero_init_residual=False,\n",
        "        groups=1,\n",
        "        widen=1,\n",
        "        width_per_group=64,\n",
        "        replace_stride_with_dilation=None,\n",
        "        norm_layer=None,\n",
        "        normalize=False,\n",
        "        output_dim=0,\n",
        "        hidden_mlp=0,\n",
        "        nmb_prototypes=0,\n",
        "        eval_mode=False,\n",
        "        first_conv=True,\n",
        "        maxpool1=True\n",
        "    ):\n",
        "        super(ResNet, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        self._norm_layer = norm_layer\n",
        "\n",
        "        self.eval_mode = eval_mode\n",
        "        self.padding = nn.ConstantPad2d(1, 0.0)\n",
        "\n",
        "        self.inplanes = width_per_group * widen\n",
        "        self.dilation = 1\n",
        "        if replace_stride_with_dilation is None:\n",
        "            # each element in the tuple indicates if we should replace\n",
        "            # the 2x2 stride with a dilated convolution instead\n",
        "            replace_stride_with_dilation = [False, False, False]\n",
        "        if len(replace_stride_with_dilation) != 3:\n",
        "            raise ValueError(\n",
        "                \"replace_stride_with_dilation should be None \"\n",
        "                \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation)\n",
        "            )\n",
        "        self.groups = groups\n",
        "        self.base_width = width_per_group\n",
        "\n",
        "        # change padding 3 -> 2 compared to original torchvision code because added a padding layer\n",
        "        num_out_filters = width_per_group * widen\n",
        "\n",
        "        if first_conv:\n",
        "            self.conv1 = nn.Conv2d(3, num_out_filters, kernel_size=7, stride=2, padding=2, bias=False)\n",
        "        else:\n",
        "            self.conv1 = nn.Conv2d(3, num_out_filters, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "\n",
        "        self.bn1 = norm_layer(num_out_filters)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        if maxpool1:\n",
        "            self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        else:\n",
        "            self.maxpool = nn.MaxPool2d(kernel_size=1, stride=1)\n",
        "\n",
        "        self.layer1 = self._make_layer(block, num_out_filters, layers[0])\n",
        "        num_out_filters *= 2\n",
        "        self.layer2 = self._make_layer(\n",
        "            block, num_out_filters, layers[1], stride=2, dilate=replace_stride_with_dilation[0]\n",
        "        )\n",
        "        num_out_filters *= 2\n",
        "        self.layer3 = self._make_layer(\n",
        "            block, num_out_filters, layers[2], stride=2, dilate=replace_stride_with_dilation[1]\n",
        "        )\n",
        "        num_out_filters *= 2\n",
        "        self.layer4 = self._make_layer(\n",
        "            block, num_out_filters, layers[3], stride=2, dilate=replace_stride_with_dilation[2]\n",
        "        )\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        # normalize output features\n",
        "        self.l2norm = normalize\n",
        "\n",
        "        # projection head\n",
        "        if output_dim == 0:\n",
        "            self.projection_head = None\n",
        "        elif hidden_mlp == 0:\n",
        "            self.projection_head = nn.Linear(num_out_filters * block.expansion, output_dim)\n",
        "        else:\n",
        "            self.projection_head = nn.Sequential(\n",
        "                nn.Linear(num_out_filters * block.expansion, hidden_mlp),\n",
        "                nn.BatchNorm1d(hidden_mlp),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Linear(hidden_mlp, output_dim),\n",
        "            )\n",
        "\n",
        "        # prototype layer\n",
        "        self.prototypes = None\n",
        "        if isinstance(nmb_prototypes, list):\n",
        "            self.prototypes = MultiPrototypes(output_dim, nmb_prototypes)\n",
        "        elif nmb_prototypes > 0:\n",
        "            self.prototypes = nn.Linear(output_dim, nmb_prototypes, bias=False)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
        "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, Bottleneck):\n",
        "                    nn.init.constant_(m.bn3.weight, 0)\n",
        "                elif isinstance(m, BasicBlock):\n",
        "                    nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
        "        norm_layer = self._norm_layer\n",
        "        downsample = None\n",
        "        previous_dilation = self.dilation\n",
        "        if dilate:\n",
        "            self.dilation *= stride\n",
        "            stride = 1\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
        "                norm_layer(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(\n",
        "            block(\n",
        "                self.inplanes,\n",
        "                planes,\n",
        "                stride,\n",
        "                downsample,\n",
        "                self.groups,\n",
        "                self.base_width,\n",
        "                previous_dilation,\n",
        "                norm_layer,\n",
        "            )\n",
        "        )\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(\n",
        "                block(\n",
        "                    self.inplanes,\n",
        "                    planes,\n",
        "                    groups=self.groups,\n",
        "                    base_width=self.base_width,\n",
        "                    dilation=self.dilation,\n",
        "                    norm_layer=norm_layer,\n",
        "                )\n",
        "            )\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward_backbone(self, x):\n",
        "        x = self.padding(x)\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        if self.eval_mode:\n",
        "            return x\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward_head(self, x):\n",
        "        if self.projection_head is not None:\n",
        "            x = self.projection_head(x)\n",
        "\n",
        "        if self.l2norm:\n",
        "            x = nn.functional.normalize(x, dim=1, p=2)\n",
        "\n",
        "        if self.prototypes is not None:\n",
        "            return x, self.prototypes(x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        if not isinstance(inputs, list):\n",
        "            inputs = [inputs]\n",
        "        idx_crops = torch.cumsum(\n",
        "            torch.unique_consecutive(\n",
        "                torch.tensor([inp.shape[-1] for inp in inputs]),\n",
        "                return_counts=True,\n",
        "            )[1], 0\n",
        "        )\n",
        "        start_idx = 0\n",
        "        for end_idx in idx_crops:\n",
        "            _out = torch.cat(inputs[start_idx:end_idx])\n",
        "\n",
        "            if 'cuda' in str(self.conv1.weight.device):\n",
        "                _out = self.forward_backbone(_out.cuda(non_blocking=True))\n",
        "            else:\n",
        "                _out = self.forward_backbone(_out)\n",
        "\n",
        "            if start_idx == 0:\n",
        "                output = _out\n",
        "            else:\n",
        "                output = torch.cat((output, _out))\n",
        "            start_idx = end_idx\n",
        "        return self.forward_head(output)\n",
        "\n",
        "\n",
        "class MultiPrototypes(nn.Module):\n",
        "\n",
        "    def __init__(self, output_dim, nmb_prototypes):\n",
        "        super(MultiPrototypes, self).__init__()\n",
        "        self.nmb_heads = len(nmb_prototypes)\n",
        "        for i, k in enumerate(nmb_prototypes):\n",
        "            self.add_module(\"prototypes\" + str(i), nn.Linear(output_dim, k, bias=False))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = []\n",
        "        for i in range(self.nmb_heads):\n",
        "            out.append(getattr(self, \"prototypes\" + str(i))(x))\n",
        "        return out\n",
        "\n",
        "\n",
        "def resnet18(**kwargs):\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n",
        "\n",
        "\n",
        "def resnet50(**kwargs):\n",
        "    return ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n",
        "\n",
        "\n",
        "def resnet50w2(**kwargs):\n",
        "    return ResNet(Bottleneck, [3, 4, 6, 3], widen=2, **kwargs)\n",
        "\n",
        "\n",
        "def resnet50w4(**kwargs):\n",
        "    return ResNet(Bottleneck, [3, 4, 6, 3], widen=4, **kwargs)\n",
        "\n",
        "\n",
        "def resnet50w5(**kwargs):\n",
        "    return ResNet(Bottleneck, [3, 4, 6, 3], widen=5, **kwargs)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5mxd3VI-sgb"
      },
      "source": [
        "#Data Transformation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulj1Ix3K_ii8"
      },
      "source": [
        "import numpy as np\n",
        "import torchvision.transforms as transforms\n",
        "import cv2\n",
        "from typing import List"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXo3m3kSUWlh"
      },
      "source": [
        "class GaussianBlur(object):\n",
        "    # Implements Gaussian blur as described in the SimCLR paper\n",
        "    def __init__(self, kernel_size, p=0.5, min=0.1, max=2.0):\n",
        "        \n",
        "        self.min = min\n",
        "        self.max = max\n",
        "\n",
        "        # kernel size is set to be 10% of the image height/width\n",
        "        self.kernel_size = kernel_size\n",
        "        self.p = p\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        sample = np.array(sample)\n",
        "\n",
        "        # blur the image with a 50% chance\n",
        "        prob = np.random.random_sample()\n",
        "\n",
        "        if prob < self.p:\n",
        "            sigma = (self.max - self.min) * np.random.random_sample() + self.min\n",
        "            sample = cv2.GaussianBlur(sample, (self.kernel_size, self.kernel_size), sigma)\n",
        "\n",
        "        return sample"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAr8qhhQUh8U"
      },
      "source": [
        "class SwAVTrainDataTransform(object):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        normalize=None,\n",
        "        size_crops: List[int] = [96, 36],\n",
        "        nmb_crops: List[int] = [2, 4],\n",
        "        min_scale_crops: List[float] = [0.33, 0.10],\n",
        "        max_scale_crops: List[float] = [1, 0.33],\n",
        "        gaussian_blur: bool = True,\n",
        "        jitter_strength: float = 1.\n",
        "    ):\n",
        "        self.jitter_strength = jitter_strength\n",
        "        self.gaussian_blur = gaussian_blur\n",
        "\n",
        "        assert len(size_crops) == len(nmb_crops)\n",
        "        assert len(min_scale_crops) == len(nmb_crops)\n",
        "        assert len(max_scale_crops) == len(nmb_crops)\n",
        "\n",
        "        self.size_crops = size_crops\n",
        "        self.nmb_crops = nmb_crops\n",
        "        self.min_scale_crops = min_scale_crops\n",
        "        self.max_scale_crops = max_scale_crops\n",
        "\n",
        "        self.color_jitter = transforms.ColorJitter(\n",
        "            0.8 * self.jitter_strength, 0.8 * self.jitter_strength, 0.8 * self.jitter_strength,\n",
        "            0.2 * self.jitter_strength\n",
        "        )\n",
        "\n",
        "        transform = []\n",
        "        color_transform = [transforms.RandomApply([self.color_jitter], p=0.8), transforms.RandomGrayscale(p=0.2)]\n",
        "\n",
        "        if self.gaussian_blur:\n",
        "            kernel_size = int(0.1 * self.size_crops[0])\n",
        "            if kernel_size % 2 == 0:\n",
        "                kernel_size += 1\n",
        "\n",
        "            color_transform.append(GaussianBlur(kernel_size=kernel_size, p=0.5))\n",
        "\n",
        "        self.color_transform = transforms.Compose(color_transform)\n",
        "\n",
        "        if normalize is None:\n",
        "            self.final_transform = transforms.ToTensor()\n",
        "        else:\n",
        "            self.final_transform = transforms.Compose([transforms.ToTensor(), normalize])\n",
        "\n",
        "        for i in range(len(self.size_crops)):\n",
        "            random_resized_crop = transforms.RandomResizedCrop(\n",
        "                self.size_crops[i],\n",
        "                scale=(self.min_scale_crops[i], self.max_scale_crops[i]),\n",
        "            )\n",
        "\n",
        "            transform.extend([\n",
        "                transforms.Compose([\n",
        "                    random_resized_crop,\n",
        "                    transforms.RandomHorizontalFlip(p=0.5), self.color_transform, self.final_transform\n",
        "                ])\n",
        "            ] * self.nmb_crops[i])\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "        # add online train transform of the size of global view\n",
        "        online_train_transform = transforms.Compose([\n",
        "            transforms.RandomResizedCrop(self.size_crops[0]),\n",
        "            transforms.RandomHorizontalFlip(), self.final_transform\n",
        "        ])\n",
        "\n",
        "        self.transform.append(online_train_transform)\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        multi_crops = list(map(lambda transform: transform(sample), self.transform))\n",
        "\n",
        "        return multi_crops"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kd_nrKXAgoNa"
      },
      "source": [
        "class SwAVEvalDataTransform(SwAVTrainDataTransform):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        normalize=None,\n",
        "        size_crops: List[int] = [96, 36],\n",
        "        nmb_crops: List[int] = [2, 4],\n",
        "        min_scale_crops: List[float] = [0.33, 0.10],\n",
        "        max_scale_crops: List[float] = [1, 0.33],\n",
        "        gaussian_blur: bool = True,\n",
        "        jitter_strength: float = 1.\n",
        "    ):\n",
        "        super().__init__(\n",
        "            normalize=normalize,\n",
        "            size_crops=size_crops,\n",
        "            nmb_crops=nmb_crops,\n",
        "            min_scale_crops=min_scale_crops,\n",
        "            max_scale_crops=max_scale_crops,\n",
        "            gaussian_blur=gaussian_blur,\n",
        "            jitter_strength=jitter_strength\n",
        "        )\n",
        "\n",
        "        input_height = self.size_crops[0]  # get global view crop\n",
        "        test_transform = transforms.Compose([\n",
        "            transforms.Resize(int(input_height + 0.1 * input_height)),\n",
        "            transforms.CenterCrop(input_height),\n",
        "            self.final_transform,\n",
        "        ])\n",
        "\n",
        "        # replace last transform to eval transform in self.transform list\n",
        "        self.transform[-1] = test_transform"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wasFMHhXTnoi"
      },
      "source": [
        "#Modell Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGEBYbRLhTEm"
      },
      "source": [
        "import math\n",
        "import os\n",
        "from warnings import warn\n",
        "\n",
        "import numpy as np\n",
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "from torch import nn\n",
        "from torch.optim.optimizer import Optimizer\n",
        "\n",
        "from typing import Callable, Optional\n",
        "from pytorch_lightning.utilities import AMPType\n",
        "\n",
        "from pl_bolts.transforms.dataset_normalizations import stl10_normalization\n",
        "from pl_bolts.optimizers.lars_scheduling import LARSWrapper"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNs2Mp63iJC5"
      },
      "source": [
        "class SwAV(pl.LightningModule):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_samples: int,\n",
        "        batch_size: int,\n",
        "        dataset: str,\n",
        "        hidden_mlp: int = 2048,\n",
        "        feat_dim: int = 128,\n",
        "        warmup_epochs: int = 10,\n",
        "        max_epochs: int = 100,\n",
        "        nmb_prototypes: int = 3000,\n",
        "        freeze_prototypes_epochs: int = 1,\n",
        "        temperature: float = 0.1,\n",
        "        sinkhorn_iterations: int = 3,\n",
        "        crops_for_assign: list = [0, 1],\n",
        "        nmb_crops: list = [2, 6],\n",
        "        first_conv: bool = True,\n",
        "        maxpool1: bool = True,\n",
        "        start_lr: float = 0.,\n",
        "        learning_rate: float = 1e-3,\n",
        "        final_lr: float = 0.,\n",
        "        weight_decay: float = 1e-6,\n",
        "        epsilon: float = 0.05,\n",
        "        **kwargs\n",
        "    ):\n",
        "       \n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "        self.dataset = dataset\n",
        "        self.num_samples = num_samples\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.hidden_mlp = hidden_mlp\n",
        "        self.feat_dim = feat_dim\n",
        "        self.nmb_prototypes = nmb_prototypes\n",
        "        self.freeze_prototypes_epochs = freeze_prototypes_epochs\n",
        "        self.sinkhorn_iterations = sinkhorn_iterations\n",
        "\n",
        "        self.crops_for_assign = crops_for_assign\n",
        "        self.nmb_crops = nmb_crops\n",
        "\n",
        "        self.first_conv = first_conv\n",
        "        self.maxpool1 = maxpool1\n",
        "\n",
        "        self.weight_decay = weight_decay\n",
        "        self.epsilon = epsilon\n",
        "        self.temperature = temperature\n",
        "\n",
        "        self.start_lr = start_lr\n",
        "        self.final_lr = final_lr\n",
        "        self.learning_rate = learning_rate\n",
        "        self.warmup_epochs = warmup_epochs\n",
        "        self.max_epochs = max_epochs\n",
        "\n",
        "        self.get_assignments = self.sinkhorn\n",
        "\n",
        "        self.model = resnet50(\n",
        "            normalize=True,\n",
        "            hidden_mlp=self.hidden_mlp,\n",
        "            output_dim=self.feat_dim,\n",
        "            nmb_prototypes=self.nmb_prototypes,\n",
        "            first_conv=self.first_conv,\n",
        "            maxpool1=self.maxpool1\n",
        "        )\n",
        "\n",
        "        # compute iters per epoch\n",
        "        self.train_iters_per_epoch = self.num_samples // self.batch_size\n",
        "\n",
        "        # define LR schedule\n",
        "        warmup_lr_schedule = np.linspace(\n",
        "            self.start_lr, self.learning_rate, self.train_iters_per_epoch * self.warmup_epochs\n",
        "        )\n",
        "        iters = np.arange(self.train_iters_per_epoch * (self.max_epochs - self.warmup_epochs))\n",
        "        cosine_lr_schedule = np.array([\n",
        "            self.final_lr + 0.5 * (self.learning_rate - self.final_lr) *\n",
        "            (1 + math.cos(math.pi * t / (self.train_iters_per_epoch * (self.max_epochs - self.warmup_epochs))))\n",
        "            for t in iters\n",
        "        ])\n",
        "\n",
        "        self.lr_schedule = np.concatenate((warmup_lr_schedule, cosine_lr_schedule))\n",
        "\n",
        "        self.queue = None\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # pass single batch from the resnet backbone\n",
        "        return self.model.forward_backbone(x)\n",
        "\n",
        "    def on_after_backward(self):\n",
        "        if self.current_epoch < self.freeze_prototypes_epochs:\n",
        "            for name, p in self.model.named_parameters():\n",
        "                if \"prototypes\" in name:\n",
        "                    p.grad = None\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(\n",
        "            self.parameters(),\n",
        "            lr=self.learning_rate\n",
        "        )\n",
        "\n",
        "        optimizer = LARSWrapper(\n",
        "                optimizer,\n",
        "                eta=0.001,  # trust coefficient\n",
        "                clip=False\n",
        "            )\n",
        "\n",
        "        return optimizer\n",
        "\n",
        "    def optimizer_step(\n",
        "        self,\n",
        "        epoch: int,\n",
        "        batch_idx: int,\n",
        "        optimizer: Optimizer,\n",
        "        optimizer_idx: int,\n",
        "        optimizer_closure: Optional[Callable] = None,\n",
        "        on_tpu: bool = False,\n",
        "        using_native_amp: bool = False,\n",
        "        using_lbfgs: bool = False,\n",
        "    ) -> None:\n",
        "        # warm-up + decay schedule placed here since LARSWrapper is not optimizer class\n",
        "        # adjust LR of optim contained within LARSWrapper\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group[\"lr\"] = self.lr_schedule[self.trainer.global_step]\n",
        "\n",
        "        # log LR (LearningRateLogger callback doesn't work with LARSWrapper)\n",
        "        self.log ('learning_rate', self.lr_schedule[self.trainer.global_step], on_step=True, on_epoch=False)\n",
        "\n",
        "        # from lightning\n",
        "        if self.trainer.amp_backend == AMPType.NATIVE:\n",
        "            optimizer_closure()\n",
        "            self.trainer.scaler.step(optimizer)\n",
        "        elif self.trainer.amp_backend == AMPType.APEX:\n",
        "            optimizer_closure()\n",
        "            optimizer.step()\n",
        "        else:\n",
        "          optimizer.step(closure=optimizer_closure)\n",
        "\n",
        "    def sinkhorn(self, Q, nmb_iters):\n",
        "        with torch.no_grad():\n",
        "            sum_Q = torch.sum(Q)\n",
        "            Q /= sum_Q\n",
        "\n",
        "            K, B = Q.shape\n",
        "\n",
        "            u = torch.zeros(K).cuda()\n",
        "            r = torch.ones(K).cuda() / K\n",
        "            c = torch.ones(B).cuda() / B\n",
        "\n",
        "            for _ in range(nmb_iters):\n",
        "                u = torch.sum(Q, dim=1)\n",
        "\n",
        "                Q *= (r / u).unsqueeze(1)\n",
        "                Q *= (c / torch.sum(Q, dim=0)).unsqueeze(0)\n",
        "\n",
        "            return (Q / torch.sum(Q, dim=0, keepdim=True)).t().float()\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        loss = self.shared_step(batch)\n",
        "\n",
        "        self.log('train_loss', loss, on_step=True, on_epoch=False)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        loss = self.shared_step(batch)\n",
        "\n",
        "        self.log('val_loss', loss, on_step=False, on_epoch=True)\n",
        "        return loss\n",
        "\n",
        "    def shared_step(self, batch):\n",
        "        if self.dataset == 'stl10':\n",
        "            unlabeled_batch = batch[0]\n",
        "            batch = unlabeled_batch\n",
        "\n",
        "        inputs, y = batch\n",
        "        inputs = inputs[:-1]  # remove online train/eval transforms at this point\n",
        "\n",
        "        # 1. normalize the prototypes\n",
        "        with torch.no_grad():\n",
        "            w = self.model.prototypes.weight.data.clone()\n",
        "            w = nn.functional.normalize(w, dim=1, p=2)\n",
        "            self.model.prototypes.weight.copy_(w)\n",
        "\n",
        "        # 2. multi-res forward passes\n",
        "        embedding, output = self.model(inputs)\n",
        "        embedding = embedding.detach()\n",
        "        bs = inputs[0].size(0)\n",
        "\n",
        "        # 3. swav loss computation\n",
        "        loss = 0\n",
        "        for i, crop_id in enumerate(self.crops_for_assign):\n",
        "            with torch.no_grad():\n",
        "                out = output[bs * crop_id:bs * (crop_id + 1)]\n",
        "\n",
        "                # 5. get assignments\n",
        "                q = torch.exp(out / self.epsilon).t()\n",
        "                q = self.get_assignments(q, self.sinkhorn_iterations)[-bs:]\n",
        "\n",
        "            # cluster assignment prediction\n",
        "            subloss = 0\n",
        "            for v in np.delete(np.arange(np.sum(self.nmb_crops)), crop_id):\n",
        "                p = self.softmax(output[bs * v:bs * (v + 1)] / self.temperature)\n",
        "                subloss -= torch.mean(torch.sum(q * torch.log(p), dim=1))\n",
        "            loss += subloss / (np.sum(self.nmb_crops) - 1)\n",
        "        loss /= len(self.crops_for_assign)\n",
        "\n",
        "        return loss"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaTL0JvzpkOA"
      },
      "source": [
        "from pl_bolts.callbacks.self_supervised import SSLOnlineEvaluator\n",
        "from pl_bolts.datamodules import STL10DataModule\n",
        "\n",
        "# args\n",
        "data_path = '.'\n",
        "dataset = 'stl10'\n",
        "batch_size = 64\n",
        "num_workers = 8\n",
        "\n",
        "# dm args\n",
        "size_crops = [96, 36]\n",
        "nmb_crops = [2, 6]\n",
        "min_scale_crops = [0.33, 0.10]\n",
        "max_scale_crops = [1, 0.33]\n",
        "gaussian_blur = True \n",
        "jitter_strength = 1.\n",
        "\n",
        "hidden_mlp = 2048\n",
        "feat_dim = 128\n",
        "max_epochs = 100\n",
        "warmup_epochs = 10\n",
        "nmb_prototypes = 512\n",
        "\n",
        "freeze_prototypes_epochs = 1\n",
        "temperature = 0.1\n",
        "learning_rate = 1e-3"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbZ_LBZBrF7O"
      },
      "source": [
        "dm = STL10DataModule(\n",
        "    data_dir=data_path, \n",
        "    batch_size=batch_size, \n",
        "    num_workers=num_workers\n",
        ")\n",
        "\n",
        "dm.train_dataloader = dm.train_dataloader_mixed\n",
        "dm.val_dataloader = dm.val_dataloader_mixed\n",
        "num_samples = dm.num_unlabeled_samples\n",
        "\n",
        "normalization = stl10_normalization()\n",
        "\n",
        "dm.train_transforms = SwAVTrainDataTransform(\n",
        "    normalize=stl10_normalization(),\n",
        "    size_crops=size_crops,\n",
        "    nmb_crops=nmb_crops,\n",
        "    min_scale_crops=min_scale_crops,\n",
        "    max_scale_crops=max_scale_crops,\n",
        "    gaussian_blur=gaussian_blur,\n",
        "    jitter_strength=jitter_strength\n",
        ")\n",
        "\n",
        "dm.val_transforms = SwAVEvalDataTransform(\n",
        "    normalize=stl10_normalization(),\n",
        "    size_crops=size_crops,\n",
        "    nmb_crops=nmb_crops,\n",
        "    min_scale_crops=min_scale_crops,\n",
        "    max_scale_crops=max_scale_crops,\n",
        "    gaussian_blur=gaussian_blur,\n",
        "    jitter_strength=jitter_strength\n",
        ")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrHSVbTJsRqL"
      },
      "source": [
        "model = SwAV(\n",
        "    num_samples=num_samples,\n",
        "    batch_size=batch_size,\n",
        "    dataset=dataset,\n",
        "    hidden_mlp=hidden_mlp,\n",
        "    feat_dim=feat_dim,\n",
        "    warmup_epochs=warmup_epochs,\n",
        "    max_epochs=max_epochs,\n",
        "    nmb_prototypes=nmb_prototypes,\n",
        "    freeze_prototypes_epochs=freeze_prototypes_epochs,\n",
        "    temperature=temperature,\n",
        "    nmb_crops=nmb_crops,\n",
        "    first_conv=True,\n",
        "    learning_rate=learning_rate\n",
        ")\n",
        "\n",
        "online_evaluator = SSLOnlineEvaluator(\n",
        "    drop_p=0.,\n",
        "    hidden_dim=None,\n",
        "    z_dim=hidden_mlp,\n",
        "    num_classes=dm.num_classes,\n",
        "#    dataset=dataset\n",
        ")"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 810,
          "referenced_widgets": [
            "ed4660e777ee4dd69f673c2af0387ba1",
            "bedb863ea3b54af29f836ae6c0b2cd60",
            "40915b1adc864cd2945c86084be328b7",
            "ec8aa2a6b07b41e6b2302e4cf644a579",
            "af02afed884d4fcc870576713283b16a",
            "8a30a539e8f54138bf5a45b9607d8903",
            "aa894fc4329c465bbb25b2377436cf42",
            "dc85f0bc159c41f2bcd6b4b6156995e4",
            "c14b88f24acf4bcfbfa8e0f34ba417d5",
            "ccabec6cf7804577a61e1f4e7f15e75c",
            "6683a03ca4ea44feb1b55ccc32d715d2",
            "2715458a09474dbaae562a9c27341877",
            "6bd912b70c30484f9044c652d378c16a",
            "96c453b0b28f416597939de2eb8bc342",
            "b4008ae9f1f8475f8c465a15626e9a9b",
            "3eec7552813d408ea84d039dceac3049",
            "a64a23f32a0342f2991638209f8c34ee",
            "dc2bf2cfce364867b83a250bb1032da8",
            "fb776bb6898d491ba6fd3ab8dccc7eec",
            "3bd955549837458a8e97833f8ba11d65",
            "810884e309b54c17be2b7b5ff542492a",
            "62d29179e0ac49ad994ed49504daa1ee",
            "98624cb2c7d244388cc1ebc815eb21c7",
            "6ce4255c08fd4f15ab1929474318c908"
          ]
        },
        "id": "7RuM8RSkuGkd",
        "outputId": "21eb2ee2-a7f9-4eca-c889-d6cf26151c17"
      },
      "source": [
        "trainer = pl.Trainer(\n",
        "    max_epochs=max_epochs,\n",
        "    gpus=1,\n",
        "    distributed_backend='ddp',\n",
        "    precision=16,\n",
        "    callbacks=[online_evaluator],\n",
        ")\n",
        "\n",
        "trainer.fit(model, dm)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: You have set progress_bar_refresh_rate < 20 on Google Colab. This may crash. Consider using progress_bar_refresh_rate >= 20 in Trainer.\n",
            "  warnings.warn(*args, **kwargs)\n",
            "GPU available: True, used: True\n",
            "TPU available: None, using: 0 TPU cores\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Using native 16bit precision.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://ai.stanford.edu/~acoates/stl10/stl10_binary.tar.gz to ./stl10_binary.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ed4660e777ee4dd69f673c2af0387ba1",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./stl10_binary.tar.gz to .\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Missing logger folder: /content/lightning_logs\n",
            "initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  | Name                 | Type         | Params\n",
            "------------------------------------------------------\n",
            "0 | model                | ResNet       | 28.0 M\n",
            "1 | softmax              | Softmax      | 0     \n",
            "2 | non_linear_evaluator | SSLEvaluator | 20.5 K\n",
            "------------------------------------------------------\n",
            "28.1 M    Trainable params\n",
            "0         Non-trainable params\n",
            "28.1 M    Total params\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c14b88f24acf4bcfbfa8e0f34ba417d5",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a64a23f32a0342f2991638209f8c34ee",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-3dfc826adb5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloader, val_dataloaders, datamodule)\u001b[0m\n\u001b[1;32m    507\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'on_fit_start'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mteardown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pytorch_lightning/accelerators/ddp_accelerator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mddp_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'WORLD_SIZE'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'WORLD_SIZE'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pytorch_lightning/accelerators/ddp_accelerator.py\u001b[0m in \u001b[0;36mddp_train\u001b[0;34m(self, process_idx, model)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;31m# train or test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_or_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;31m# clean up memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pytorch_lightning/accelerators/accelerator.py\u001b[0m in \u001b[0;36mtrain_or_test\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    558\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_training_epoch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m                     \u001b[0;31m# run train epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_training_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_steps\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_steps\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py\u001b[0m in \u001b[0;36mrun_training_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0;31m# hook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0;31m# TODO: add outputs to batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_end_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0;31m# -----------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/training_loop.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, epoch_output, epoch_end_outputs, batch, batch_idx, dataloader_idx)\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;31m# hook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'on_batch_end'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'on_train_batch_end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_end_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0;31m# figure out what to track for epoch end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mcall_hook\u001b[0;34m(self, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    922\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m                 \u001b[0mtrainer_hook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m                 \u001b[0mtrainer_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m             \u001b[0;31m# next call hook in lightningModule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pytorch_lightning/trainer/callback_hook.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, outputs, batch, batch_idx, dataloader_idx)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;34m\"\"\"Called when the training batch ends.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_validation_batch_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: on_train_batch_end() takes 6 positional arguments but 7 were given"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xz9KCRTNxg4V"
      },
      "source": [
        "# Start tensorboard.\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir lightning_logs/"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}